@book{Teugels,
	author={Jef L. Teugels and Bjrn Sundt},
	year={2004},
	title={Encyclopedia of actuarial science},
	publisher={Hoboken, NJ : John Wiley and Sons},
	address={Hoboken, NJ},
	note={Includes bibliographical references and index}
}
@misc{Young,
	author = 	 {Jan Nygaard Nielsen and Henrik Madsen and Peter C. Young},
	year = 	 {2000},
	title = 	 {Parameter estimation in stochastic differential equations: An overview},
	journal = 	 {Annual Reviews in Control},
	volume = 	 {24},
	pages = 	 {83-94},
	note = 	 {ID: 271897271897},
	abstract = 	 {This paper presents an overview of the progress of research on parameter estimation methods for stochastic differential equations (mostly in the sense of Itô calculus) over the period 1981–1999. These are considered both without measurement noise and with measurement noise, where the discretely observed stochastic differential equations are embedded in a continuous-discrete time state space model. Every attempts has been made to include results from other scientific disciplines. Maximum likelihood estimation of parameters in nonlinear stochastic differential equations is in general not possible due to the unavailability of closed form expressions for the transition and stationary probability density functions of the states. However, major developments are classified according to their approximation to the “true” maximum likelihood solution as opposed to a historical order of presentation. "},
	isbn = 	 {1367-5788},
	url = 	 {http://www.sciencedirect.com/science/article/pii/S1367578800900178},
	doi={//dx.doi.org/10.1016/S1367-5788(00)90017-8}
}
@misc{Samad,
	author = 	 {Tariq Samad and Anoop Mathur},
	year = 	 {1992},
	title = 	 {Parameter estimation for process control with neural networks},
	journal = 	 {International Journal of Approximate Reasoning},
	volume = 	 {7},
	number = 	 {3},
	pages = 	 {149-164},
	note = 	 {ID: 271876271876},
	abstract = 	 {Neural networks are applied to the problem of parameter estimation for process systems. Neural network parameter estimators for a given parametrized model structure can be developed by supervised learning. Training examples can be dynamically generated by using a process simulation, resulting in trained networks that are capable of high generalization. This approach can be used for a variety of parameter estimation applications. A proof-of-concept open-loop delay estimator is described, and extensive simulation results are detailed. Some results of other parameter estimation networks are also given. Extensions to recursive and closed-loop identification and application to higher-order processes are discussed. "},
	isbn = 	 {0888-613X},
	url = 	 {http://www.sciencedirect.com/science/article/pii/0888613X9290008N},
	doi={//dx.doi.org/10.1016/0888-613X(92)90008-N}
}
@article{Werbos,
	author={Paul J. Werbos},
	year={1990},
	title={Backpropagation through time: what it does and how to do it},
	journal={Proceedings of the IEEE},
	volume={78},
	number={10},
	pages={1550-1560}
}
@misc{Samad,
	author = 	 {Tariq Samad and Anoop Mathur},
	year = 	 {1992},
	title = 	 {Parameter estimation for process control with neural networks},
	journal = 	 {International Journal of Approximate Reasoning},
	volume = 	 {7},
	number = 	 {3},
	pages = 	 {149-164},
	note = 	 {ID: 271876271876},
	abstract = 	 {Neural networks are applied to the problem of parameter estimation for process systems. Neural network parameter estimators for a given parametrized model structure can be developed by supervised learning. Training examples can be dynamically generated by using a process simulation, resulting in trained networks that are capable of high generalization. This approach can be used for a variety of parameter estimation applications. A proof-of-concept open-loop delay estimator is described, and extensive simulation results are detailed. Some results of other parameter estimation networks are also given. Extensions to recursive and closed-loop identification and application to higher-order processes are discussed. "},
	isbn = 	 {0888-613X},
	url = 	 {http://www.sciencedirect.com/science/article/pii/0888613X9290008N},
	doi={//dx.doi.org/10.1016/0888-613X(92)90008-N}
}
@book{Kotz,
	author={Samuel Kotz and Campbell B. Read and Norman Lloyd Johnson},
	year={1982},
	title={Encyclopedia of statistical sciences},
	publisher={New York, N.Y. : Wiley},
	address={New York, N.Y.},
	note={Bibliographical references.; ID: 27US_ALMA2132772640003436}
}
@book{Sundt,
	author={Jef L. Teugels and Bjørn Sundt},
	year={2004},
	title={Encyclopedia of actuarial science},
	publisher={Hoboken, NJ : John Wiley and Sons},
	address={Hoboken, NJ},
	note={Includes bibliographical references and index}
}
@misc{Merton,
	author = 	 {Robert C. Merton},
	year = 	 {1976},
	title = 	 {Option pricing when underlying stock returns are discontinuous},
	journal = 	 {Journal of Financial Economics},
	volume = 	 {3},
	number = 	 {1},
	pages = 	 {125-144},
	note = 	 {ID: 271671271671},
	abstract = 	 {The validity of the classic Black-Scholes option pricing formula depends on the capability of investors to follow a dynamic portfolio strategy in the stock that replicates the payoff structure to the option. The critical assumption required for such a strategy to be feasible, is that the underlying stock return dynamics can be described by a stochastic process with a continuous sample path. In this paper, an option pricing formula is derived for the more-general case when the underlying stock returns are generated by a mixture of both continuous and jump processes. The derived formula has most of the attractive features of the original Black-Scholes formula in that it does not depend on investor preferences or knowledge of the expected return on the underlying stock. Moreover, the same analysis applied to the options can be extended to the pricing of corporate liabilities. "},
	isbn = 	 {0304-405X},
	url = 	 {http://www.sciencedirect.com.ez.sun.ac.za/science/article/pii/0304405X76900222},
	doi={//dx.doi.org.ez.sun.ac.za/10.1016/0304-405X(76)90022-2}
}
@misc{Salkind,
	author = 	 {Neil Salkind},
	year = 	 {2017},
	title = 	 {Encyclopedia of Measurement and Statistics},
	url = 	 {http://methods.sagepub.com/reference/encyclopedia-of-measurement-and-statistics},
	doi={10.4135/9781412952644}
}
@article{Oja,
	author={Erkki Oja},
	year={1992},
	title={Principal components, minor components, and linear neural networks},
	journal={Neural Networks},
	volume={5},
	number={6},
	pages={927-935}
}
@phdthesis{Hugo,
	author={Carina Hugo},
	year={2006},
	title={Die kulkuns van die letterdief : 'n ondersoek na plagiaat in die Suid-Afrikaanse gedrukte media, met spesiale verwysing na drie onlangse gevallestudies},
	journal={Kulkuns vd letterdief 'n ondersoek na plagiaat},
	note={Assignment (MPhil)--University of Stellenbosch, 2006. ; Bibliography.; ID: dedupmrg7372831}
}
@article{Oja,
	author={Erkki Oja},
	year={1992},
	title={Principal components, minor components, and linear neural networks},
	journal={Neural Networks},
	volume={5},
	number={6},
	pages={927-935}
}
@misc{Sherrod,
	author={Phil Sherrod},
	year={2017},
	title={Multilayer Perceptron Neural Networks},
	volume={2017},
	number={8 February},
	abstract={Neural networks are predictive models loosely based on the action of biological neurons.},
	url={https://www.dtreg.com/solution/view/21}
}
@article{Timmer,
	author={J. Timmer},
	year={2000},
	title={Parameter estimation in nonlinear stochastic differential equations},
	journal={Chaos, Solitons and Fractals},
	volume={11},
	number={15},
	pages={2571-2578},
	abstract={We discuss the problem of parameter estimation in nonlinear stochastic differential equations (SDEs) based on sampled time series. A central message from the theory of integrating SDEs is that there exist in general two time scales, i.e. that of integrating these equations and that of sampling. We argue that therefore, maximum likelihood estimation is computationally extremely expensive. We discuss the relation between maximum likelihood and quasi maximum likelihood estimation. In a simulation study, we compare the quasi maximum likelihood method with an approach for parameter estimation in nonlinear SDEs that disregards the existence of the two time scales.},
	isbn={0960-0779},
	url={//www.sciencedirect.com/science/article/pii/S0960077900000151},
	doi={//dx.doi.org/10.1016/S0960-0779(00)00015-1}
}
@misc{Nielsen,
	author = 	 {Jan Nygaard Nielsen and Henrik Madsen and Peter C. Young},
	year = 	 {2000},
	title = 	 {Parameter estimation in stochastic differential equations: An overview},
	journal = 	 {Annual Reviews in Control},
	volume = 	 {24},
	pages = 	 {83-94},
	note = 	 {ID: 271897271897},
	abstract = 	 {This paper presents an overview of the progress of research on parameter estimation methods for stochastic differential equations (mostly in the sense of Itô calculus) over the period 1981–1999. These are considered both without measurement noise and with measurement noise, where the discretely observed stochastic differential equations are embedded in a continuous-discrete time state space model. Every attempts has been made to include results from other scientific disciplines. Maximum likelihood estimation of parameters in nonlinear stochastic differential equations is in general not possible due to the unavailability of closed form expressions for the transition and stationary probability density functions of the states. However, major developments are classified according to their approximation to the “true” maximum likelihood solution as opposed to a historical order of presentation. "},
	isbn = 	 {1367-5788},
	url = 	 {http://www.sciencedirect.com/science/article/pii/S1367578800900178},
	doi={//dx.doi.org/10.1016/S1367-5788(00)90017-8}
}
@inbook{Barone-Adesi,
	author={Giovanni Barone-Adesi},
	year={2015},
	title={Stochastic Processes},
	series={Wiley Encyclopedia of Management},
	publisher={John Wiley and Sons, Ltd},
	isbn={9781118785317},
	url={http://dx.doi.org/10.1002/9781118785317.weom040071},
	doi={10.1002/9781118785317.weom040071}
}
@article{Hornik,
	author={Kurt Hornik and Maxwell Stinchcombe and Halbert White},
	year={1989},
	title={Multilayer feedforward networks are universal approximators},
	journal={Neural Networks},
	volume={2},
	number={5},
	pages={359-366},
	abstract={This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	isbn={0893-6080},
	url={//www.sciencedirect.com/science/article/pii/0893608089900208},
	doi={//dx.doi.org/10.1016/0893-6080(89)90020-8}
}
@article{Csji,
	author={Balzs Csand Csji},
	year={2001},
	title={Approximation with artificial neural networks},
	journal={Faculty of Sciences, Etvs Lornd University, Hungary},
	volume={24},
	pages={48}
}
@article{Bennell,
	author={Julia Bennell and Charles Sutcliffe},
	year={2004},
	title={Black–Scholes versus artificial neural networks in pricing FTSE 100 options},
	journal={Intelligent Systems in Accounting, Finance & Management},
	volume={12},
	number={4},
	pages={243-260},
	abstract={This paper compares the performance of Black–Scholes with an artificial neural network (ANN) in pricing European-style call options on the FTSE 100 index. It is the first extensive study of the performance of ANNs in pricing UK options, and the first to allow for dividends in the closed-form model. For out-of-the-money options, the ANN is clearly superior to Black–Scholes. For in-the-money options, if the sample space is restricted by excluding deep in-the-money and long maturity options (3.4% of total volume), then the performance of the ANN is comparable to that of Black–Scholes. The superiority of the ANN is a surprising result, given that European-style equity options are the home ground of Black–Scholes, and suggests that ANNs may have an important role to play in pricing other options for which there is either no closed-form model, or the closed-form model is less successful than is Black–Scholes for equity options. Copyright © 2005 John Wiley and Sons, Ltd.},
	isbn={1099-1174},
	url={http://dx.doi.org/10.1002/isaf.254},
	doi={10.1002/isaf.254}
}
@inproceedings{Malliaris,
	author={M. Malliaris and L. Salchenberger},
	year={1993},
	title={Beating the best: A neural network challenges the Black-Scholes formula},
	booktitle={Proceedings of 9th IEEE Conference on Artificial Intelligence for Applications},
	pages={445-449},
	note={ID: 1},
	doi={10.1109/CAIA.1993.366633}
}
@article{Yao,
	author={Jingtao Yao and Yili Li and Chew Lim Tan},
	year={2000},
	title={Option price forecasting using neural networks},
	journal={Omega},
	volume={28},
	number={4},
	pages={455-466},
	abstract={In this research, forecasting of the option prices of Nikkei 225 index futures is carried out using backpropagation neural networks. Different results in terms of accuracy are achieved by grouping the data differently. The results suggest that for volatile markets a neural network option pricing model outperforms the traditional Black–Scholes model. However, the Black–Scholes model is still good for pricing at-the-money options. In using the neural network model, data partition according to moneyness should be applied. Those who prefer less risk and less returns may use the traditional Black–Scholes model results while those who prefer high risk and high return may choose to use the neural network model results.},
	isbn={0305-0483},
	url={//www.sciencedirect.com/science/article/pii/S0305048399000663},
	doi={//dx.doi.org/10.1016/S0305-0483(99)00066-3}
}
@article{Giebel,
	author={Stefan Giebel and Martin Rainer},
	year={2013},
	title={Neural network calibrated stochastic processes: forecasting financial assets},
	journal={Central European Journal of Operations Research},
	volume={21},
	number={2},
	pages={277-293}
}
@article{Xie,
	author={Z. Xie and D. Kulasiri and S. Samarasinghe and C. Rajanayaka},
	year={2007},
	title={The estimation of parameters for stochastic differential equations using neural networks},
	journal={Inverse Problems in Science and Engineering},
	volume={15},
	number={6},
	pages={629-641},
	note={doi: 10.1080/17415970600907429},
	isbn={1741-5977},
	url={http://dx.doi.org.ez.sun.ac.za/10.1080/17415970600907429},
	doi={10.1080/17415970600907429}
}
@article{Olden,
	author={Julian D. Olden and Donald A. Jackson},
	year={2002},
	title={Illuminating the "black box": a randomization approach for understanding variable contributions in artificial neural networks},
	journal={Ecological Modelling},
	volume={154},
	number={1-2},
	pages={135-150},
	isbn={0304-3800},
	url={//www.sciencedirect.com/science/article/pii/S0304380002000649},
	doi={//dx.doi.org/10.1016/S0304-3800(02)00064-9}
}
@article{Esteva,
	author={Andre Esteva and Brett Kuprel and Roberto A. Novoa and Justin Ko and Susan M. Swetter and Helen M. Blau and Sebastian Thrun},
	year={2017},
	title={Dermatologist-level classification of skin cancer with deep neural networks},
	journal={Nature}
}
@inproceedings{Hinton,
	author={Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
	year={2012},
	title={Imagenet classification with deep convolutional neural networks},
	booktitle={Advances in neural information processing systems},
	pages={1097-1105}
}
@article{Mongwe,
	author={Wilson Tsakane Mongwe},
	year={2015},
	title={No title},
	journal={Analysis of equity and interest rate returns in South Africa under the context of jump diffusion processes}
}
@article{Honore,
	author={Peter Honore},
	year={1998},
	title={Pitfalls in estimating jump-diffusion models}
}
@article{Oreskes,
	author={Naomi Oreskes and Kristin Shrader-Frechette and Kenneth Belitz},
	year={1994},
	title={Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences},
	journal={Science},
	volume={263},
	number={5147},
	pages={641-646},
	abstract={Verification and validation of numerical models of natural systems is impossible. This is because natural systems are never closed and because model results are always non-unique. Models can be confirmed by the demonstration of agreement between observation and prediction, but confirmation is inherently partial. Complete confirmation is logically precluded by the fallacy of affirming the consequent and by incomplete access to natural phenomena. Models can only be evaluated in relative terms, and their predictive value is always open to question. The primary value of models is heuristic.},
	isbn={0036-8075,10959203},
	url={http://www.jstor.org/stable/2883078}
}
@article{Fararo,
	author={Thomas J. Fararo},
	year={1969},
	title={Stochastic Processes},
	journal={Sociological Methodology},
	volume={1},
	pages={245-260},
	isbn={0081-1750,14679531},
	url={http://www.jstor.org.ez.sun.ac.za/stable/270886},
	doi={10.2307/270886}
}
@article{cairns1998stochastic,
	title={Stochastic processes: learning the language},
	author={Cairns, AJG and Dickson, DCM and Macdonald, AS and Waters, HR and Willder, M},
	journal={Faculty of Actuaries students' society},
	year={1998}
}
@online{cs231n,
	ALTauthor = {Andrej Karpathy},
	ALTeditor = {Stanford University CS231n},
	title = {CS231n Convolutional Neural Networks for Visual Recognition},
	date = {2017},
	url = {http://cs231n.github.io/convolutional-networks/},
	OPTsubtitle = {Convolutional Neural Networks (CNNs / ConvNets)},
	OPTlanguage = {english},
	OPTorganization = {Stanford University}
}

@article{DBLP:journals/corr/ClevertUH15,
	author    = {Djork{-}Arn{\'{e}} Clevert and
	Thomas Unterthiner and
	Sepp Hochreiter},
	title     = {Fast and Accurate Deep Network Learning by Exponential Linear Units
	(ELUs)},
	journal   = {CoRR},
	volume    = {abs/1511.07289},
	year      = {2015},
	url       = {http://arxiv.org/abs/1511.07289},
	timestamp = {Wed, 07 Jun 2017 14:40:17 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ClevertUH15},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{glorot2011deep,
	title={Deep Sparse Rectifier Neural Networks},
	author={Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages={315--323},
	year={2011}
}

@Inbook{Scherer2010,
	author="Scherer, Dominik
	and M{\"u}ller, Andreas
	and Behnke, Sven",
	editor="Diamantaras, Konstantinos
	and Duch, Wlodek
	and Iliadis, Lazaros S.",
	title="Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition",
	bookTitle="Artificial Neural Networks -- ICANN 2010: 20th International Conference, Thessaloniki, Greece, September 15-18, 2010, Proceedings, Part III",
	year="2010",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="92--101",
	abstract="A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57{\%} on the NORB normalized-uniform dataset and 5.6{\%} on the NORB jittered-cluttered dataset.",
	isbn="978-3-642-15825-4",
	doi="10.1007/978-3-642-15825-4_10",
	url="http://dx.doi.org/10.1007/978-3-642-15825-4_10"
}

@article{adam,
	author    = {Diederik P. Kingma and
	Jimmy Ba},
	title     = {Adam: {A} Method for Stochastic Optimization},
	journal   = {CoRR},
	volume    = {abs/1412.6980},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.6980},
	timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KingmaB14},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{reid,
	author={Stuart G. Reid},
	year={2015},
	journal ={Turing Finance},
	title={Random walks down Wall Street, Stochastic Processes in Python},
	url={http://www.turingfinance.com/random-walks-down-wall-street-stochastic-processes-in-python/}
}

@misc{tensorflow2015-whitepaper,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={http://tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{\i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dan~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}

@misc{chollet2015keras,
	title={Keras},
	author={Chollet, Fran\c{c}ois and others},
	year={2015},
	publisher={GitHub},
	howpublished={\url{https://github.com/fchollet/keras}},
}

@misc{numpy,
	author =    {Travis Oliphant and others},
	title =     {{Numpy}: Open source array tools for {Python}},
	year =      {2001--},
	url = "http://www.numpy.org/",
	note = {[Online; accessed <today>]}
}

@inproceedings{statsmodels,
	title={Statsmodels: Econometric and statistical modeling with python},
	author={Seabold, Skipper and Perktold, Josef},
	booktitle={9th Python in Science Conference},
	year={2010},
}

@misc{scipy,
	author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
	title =     {{SciPy}: Open source scientific tools for {Python}},
	year =      {2001--},
	url = "http://www.scipy.org/",
	note = {[Online; accessed <today>]}
}

@techreport{python,
	author = {Rossum, Guido},
	title = {Python Reference Manual},
	year = {1995},
	source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aercim_cwi%3Aercim.cwi%2F%2FCS-R9525},
	publisher = {CWI (Centre for Mathematics and Computer Science)},
	address = {Amsterdam, The Netherlands, The Netherlands},
} 


@misc{seaborn,
	author       = {Michael Waskom and
	Olga Botvinnik and
	drewokane and
	Paul Hobson and
	David and
	Yaroslav Halchenko and
	Saulius Lukauskas and
	John B. Cole and
	Jordi Warmenhoven and
	Julian de Ruiter and
	Stephan Hoyer and
	Jake Vanderplas and
	Santi Villalba and
	Gero Kunter and
	Eric Quintero and
	Marcel Martin and
	Alistair Miles and
	Kyle Meyer and
	Tom Augspurger and
	Tal Yarkoni and
	Pete Bachant and
	Mike Williams and
	Constantine Evans and
	Clark Fitzgerald and
	Brian and
	Daniel Wehner and
	Gregory Hitz and
	Erik Ziegler and
	Adel Qalieh and
	Antony Lee},
	title        = {seaborn: v0.7.1 (June 2016)},
	month        = jun,
	year         = 2016,
	doi          = {10.5281/zenodo.54844},
	url          = {https://doi.org/10.5281/zenodo.54844}
}

@Article{matplotlib,
	Author    = {Hunter, J. D.},
	Title     = {Matplotlib: A 2D graphics environment},
	Journal   = {Computing In Science \& Engineering},
	Volume    = {9},
	Number    = {3},
	Pages     = {90--95},
	abstract  = {Matplotlib is a 2D graphics package used for Python
	for application development, interactive scripting, and
	publication-quality image generation across user
	interfaces and operating systems.},
	publisher = {IEEE COMPUTER SOC},
	doi = {10.1109/MCSE.2007.55},
	year      = 2007
}

@Article{ipython,
	Author    = {P\'erez, Fernando and Granger, Brian E.},
	Title     = {{IP}ython: a System for Interactive Scientific Computing},
	Journal   = {Computing in Science and Engineering},
	Volume    = {9},
	Number    = {3},
	Pages     = {21--29},
	month     = may,
	year      = 2007,
	url       = "http://ipython.org",
	ISSN      = "1521-9615",
	doi       = {10.1109/MCSE.2007.53},
	publisher = {IEEE Computer Society},
}